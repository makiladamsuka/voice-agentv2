from dotenv import load_dotenv
from livekit import agents, rtc
from livekit.agents import Agent, AgentSession, RunContext
from livekit.agents.llm import function_tool
from livekit.plugins import openai, deepgram, silero
import os
import pickle
import json
import asyncio
import re
import signal
from pathlib import Path
from image_manager import ImageManager
from image_server import ImageServer
from face_monitor import FaceMonitor
from object_detector import ObjectDetector
from greetings import generate_greeting, generate_group_greeting
from event_database import EventDatabase, build_event_database
from emotion_parser import parse_emotion, get_emotion_for_context
from emotion_sync import get_emotion_for_text, analyze_emotion
import oled_display
from oled_display import EmotionMode

# Import modular tools
from tools.vision import VisionTools
from tools.content import ContentTools
from tools.system import SystemTools

# Load environment variables
env_path = Path(__file__).parent / ".env"
load_dotenv(env_path)


class EmotionSpeechWrapper:
    """
    Wrapper that provides emotional speech with VADER sentiment analysis.
    Analyzes text segments and syncs OLED emotions with speech.
    """
    
    @staticmethod
    async def speak_with_emotion(session, text: str):
        """
        Speak text with synchronized emotions.
        Analyzes each sentence and shows matching emotion while speaking.
        
        Args:
            session: AgentSession to use for speaking
            text: Full text to speak
        """
        # Get emotionally segmented text
        segments = get_emotion_for_text(text)
        
        print(f"\nğŸ­ === EMOTION SYNC DEBUG ===")
        print(f"ğŸ“ Full text: {text}")
        print(f"ğŸ“Š Segments: {len(segments)}")
        for i, seg in enumerate(segments):
            print(f"   {i+1}. [{seg['emotion']}] {seg['text']}")
        print(f"ğŸ­ ===========================\n")
        
        for segment in segments:
            emotion = segment["emotion"]
            segment_text = segment["text"]
            
            print(f"ğŸ¤ NOW SPEAKING: [{emotion}] {segment_text}")
            
            # Start emotion (looping mode) - DISABLED here, handled by tts_node for better sync
            # try:
            #     if oled_display.DISPLAY_RUNNING:
            #         oled_display.start_emotion(emotion)
            #         print(f"ğŸ‘€ OLED: Started {emotion} emotion")
            # except Exception as e:
            #     print(f"âš ï¸ OLED error: {e}")
            
            # Speak the segment
            try:
                await session.say(segment_text)
            except Exception as e:
                print(f"âš ï¸ Speech error: {e}")
            
            # Small pause between segments
            await asyncio.sleep(0.1)
        
        # Return to idle after all speech - DISABLED here, handled by tts_node and session events
        # try:
        #     if oled_display.DISPLAY_RUNNING:
        #         oled_display.stop_emotion()
        #         print(f"ğŸ‘€ OLED: Returned to idle")
        # except Exception as e:
        #     print(f"âš ï¸ OLED error: {e}")


# Convenience function for easier use
async def emotional_say(session, text: str):
    """Convenience function to speak with emotions."""
    await EmotionSpeechWrapper.speak_with_emotion(session, text)


class CampusGreetingAgent(Agent):
    def __init__(self, image_server, event_db=None):
        # Initialize image manager
        assets_dir = Path(__file__).parent / "assets"
        self.image_manager = ImageManager(assets_dir)
        self.image_server = image_server
        self.event_db = event_db  # Event database for Q&A
        self.face_monitor = None
        self._object_detector = None
        
        # Room reference
        self.room = None
        
        # Initialize tool helpers
        self.vision_tools = VisionTools(
            face_monitor=None,  # Will set later
            object_detector_factory=self.get_object_detector
        )
        self.content_tools = ContentTools(
            image_manager=self.image_manager,
            image_server=self.image_server,
            room_provider=lambda: self.room
        )
        self.system_tools = SystemTools()
        
        # Load known face encodings
        self.known_faces = {}
        encodings_path = Path(__file__).parent / "known_faces" / "encodings.pkl"
        
        if encodings_path.exists():
            try:
                with open(encodings_path, 'rb') as f:
                    self.known_faces = pickle.load(f)
                print(f"âœ… Loaded face encodings for {len(self.known_faces)} people: {list(self.known_faces.keys())}")
            except (ModuleNotFoundError, AttributeError, ImportError) as e:
                # Handle numpy version incompatibility or other pickle loading issues
                print(f"âš ï¸  Failed to load face encodings due to version incompatibility: {e}")
                print("   The encodings.pkl file may have been created with a different numpy version.")
                print("   Face recognition will start fresh. Re-enroll faces if needed.")
                # Backup the old file and create a new empty one
                backup_path = encodings_path.with_suffix('.pkl.backup')
                try:
                    import shutil
                    shutil.move(encodings_path, backup_path)
                    print(f"   Old encodings backed up to: {backup_path}")
                except Exception as backup_error:
                    print(f"   Could not backup old encodings: {backup_error}")
                self.known_faces = {}
            except Exception as e:
                print(f"âš ï¸  Error loading face encodings: {e}")
                print("   Face recognition will start fresh.")
                self.known_faces = {}
        else:
            print("âš ï¸  No face encodings found. Face recognition will be limited.")
        
        super().__init__(
            instructions="""You are a friendly campus assistant robot with continuous face recognition.

ABOUT YOUR EMOTIONS:
Your facial expressions are AUTOMATIC! They sync with what you say.
- Say something happy â†’ happy eyes appear
- Say "sorry" â†’ sad eyes appear
- No need to manage emotions - just speak naturally!

CRITICAL CAPABILITY:
I can ALWAYS see who is in front of me. Their name appears in system messages.

AUTO-ENROLLMENT:
When someone introduces themselves (says "I'm [Name]" or "My name is [Name]"):
1. IMMEDIATELY call enroll_new_face(their_name) 
2. Then respond warmly: "Nice to meet you, [Name]! I'll remember you now."

NAME USAGE:
- Use names occasionally and naturally, not every response
- For UNKNOWN: "Hi there! I don't think we've met. What's your name?"
- For KNOWN: "Hey [Name]! Good to see you!"

CONVERSATION STYLE:
- Short, clear sentences
- Friendly, warm tone
- Be helpful and responsive

AVAILABLE TOOLS:
- identify_color, identify_object, count_people_in_room, describe_environment
- show_event_poster, show_location_map, ask_about_events
- enroll_new_face(name)
- get_cpu_temperature, get_system_info, get_cpu_usage, get_memory_usage"""
        )
    
    def get_object_detector(self):
        """Lazy-load ObjectDetector"""
        if self._object_detector is None:
            print("ğŸ” Loading YOLO model...")
            self._object_detector = ObjectDetector()
        return self._object_detector
    
    async def tts_node(self, text_stream, model_settings):
        """
        Override tts_node to intercept ALL text going to TTS.
        Applies VADER sentiment analysis for real-time emotion sync.
        
        TIMING: Emotion triggers on FIRST AUDIO FRAME, not text input.
        This syncs emotion with actual speech output.
        """
        accumulated_text = ""
        detected_emotion = "idle1"
        emotion_triggered = False
        chunk_count = 0
        audio_frame_count = 0
        
        print("\nğŸ­ === TTS_NODE STARTED ===")
        print(f"ğŸ‘€ OLED Running: {oled_display.DISPLAY_RUNNING}")
        
        async def emotion_aware_text_stream():
            nonlocal accumulated_text, detected_emotion, chunk_count
            
            async for text_chunk in text_stream:
                chunk_count += 1
                accumulated_text += text_chunk
                
                # Analyze emotion but DON'T trigger yet (wait for audio)
                detected_emotion = analyze_emotion(accumulated_text)
                print(f"ğŸ“ Chunk {chunk_count}: '{text_chunk}' â†’ emotion: [{detected_emotion}]")
                
                yield text_chunk
            
            print(f"\nğŸ“ TTS Text Complete: {accumulated_text[:60]}...")
            print(f"ğŸ­ Detected emotion: [{detected_emotion}]")
        
        # Call parent's tts_node with our emotion-aware stream
        async for audio_frame in super().tts_node(emotion_aware_text_stream(), model_settings):
            audio_frame_count += 1
            
            # Trigger emotion on FIRST audio frame (when speech actually starts)
            if not emotion_triggered and detected_emotion != "idle1":
                emotion_triggered = True
                print(f"ğŸ”Š Audio frame #{audio_frame_count} - TRIGGERING EMOTION: [{detected_emotion}]")
                try:
                    if oled_display.DISPLAY_RUNNING:
                        oled_display.start_emotion(detected_emotion)
                        print(f"ğŸ‘€ OLED: {detected_emotion}")
                except Exception as e:
                    print(f"âš ï¸ OLED error: {e}")
            
            yield audio_frame
        
        # Return to idle after ALL audio frames sent
        print(f"ğŸ”Š Audio complete ({audio_frame_count} frames)")
        
        # Safety watchdog: return to idle after buffer clears
        async def safety_return_to_idle():
            # Wait for audio buffer to clear (1.5s - 2.5s is safe for typical V/A sync)
            await asyncio.sleep(2.0) 
            try:
                # ONLY if the agent is not in another speech session
                # This check is basic but helps with back-to-back segments
                if oled_display.DISPLAY_RUNNING:
                    oled_display.stop_emotion()
                    print("ğŸ‘€ OLED: Safety fallback returned to idle")
            except: pass
            
        asyncio.create_task(safety_return_to_idle())

    # --- Delegate to Tool Modules ---

    @function_tool
    async def recognize_face(self, context: RunContext) -> str:
        """Identifies who is currently in front of the webcam."""
        print("ğŸ¥ [TOOL] recognize_face called")
        self.vision_tools.face_monitor = self.face_monitor
        return await self.vision_tools.recognize_face(context)
    
    @function_tool
    async def enroll_new_face(self, person_name: str, context: RunContext) -> str:
        """Enroll a new person's face for recognition."""
        print(f"ğŸ“ [TOOL] enroll_new_face called for: {person_name}")
        self.vision_tools.face_monitor = self.face_monitor
        return await self.vision_tools.enroll_new_face(person_name, context)

    @function_tool
    async def identify_color(self, context: RunContext) -> str:
        """Identifies the dominant color in the camera view."""
        print("ğŸ¨ [TOOL] identify_color called")
        self.vision_tools.face_monitor = self.face_monitor
        return await self.vision_tools.identify_color(context)
    
    @function_tool
    async def describe_environment(self, context: RunContext) -> str:
        """Describes the current environment - people count and visible objects."""
        print("ğŸ‘ï¸ [TOOL] describe_environment called")
        self.vision_tools.face_monitor = self.face_monitor
        return await self.vision_tools.describe_environment(context)
    
    @function_tool
    async def identify_object(self, object_name: str, context: RunContext) -> str:
        """Finds a specific object and describes it."""
        print(f"ğŸ” [TOOL] identify_object called for: {object_name}")
        self.vision_tools.face_monitor = self.face_monitor
        return await self.vision_tools.identify_object(object_name, context)
    
    @function_tool
    async def count_people_in_room(self, context: RunContext) -> str:
        """Counts how many people are visible in the camera view."""
        print("ğŸ‘¥ [TOOL] count_people_in_room called")
        self.vision_tools.face_monitor = self.face_monitor
        return await self.vision_tools.count_people_in_room(context)
    
    @function_tool
    async def set_emotion(self, emotion: str, context: RunContext) -> str:
        """
        Set the robot's emotional expression on the OLED eyes.
        MUST be called before every response to show the appropriate emotion.
        
        Args:
            emotion: One of: idle, happy, smile, looking, sad, angry, boring
        """
        print(f"ğŸ˜Š [TOOL] set_emotion called: {emotion}")
        
        valid_emotions = ["idle", "happy", "smile", "looking", "sad", "angry", "boring"]
        emotion_clean = emotion.lower().strip()
        
        if emotion_clean not in valid_emotions:
            return f"Invalid emotion. Use one of: {', '.join(valid_emotions)}"
        
        # Trigger OLED emotion display
        try:
            if oled_display.DISPLAY_RUNNING:
                oled_display.display_emotion(emotion_clean)
                return f"Emotion set to: {emotion_clean}"
            else:
                return f"Emotion received: {emotion_clean} (display not running)"
        except Exception as e:
            print(f"âš ï¸ OLED error: {e}")
            return f"Emotion received: {emotion_clean} (display error)"

    @function_tool
    async def list_available_events(self, context: RunContext) -> str:
        """Lists all available events on campus."""
        print("ğŸ“‹ [TOOL] list_available_events called")
        return await self.content_tools.list_available_events(context)
    
    @function_tool
    async def show_event_poster(self, event_description: str, context: RunContext) -> str:
        """Displays an event poster on the frontend."""
        print(f"ğŸ¨ [TOOL] show_event_poster called for: {event_description}")
        return await self.content_tools.show_event_poster(event_description, context)
    
    @function_tool
    async def show_location_map(self, location_query: str, context: RunContext) -> str:
        """Displays a campus location map on the frontend."""
        print(f"ğŸ—ºï¸ [TOOL] show_location_map called for: {location_query}")
        return await self.content_tools.show_location_map(location_query, context)
    
    @function_tool
    async def get_cpu_temperature(self, context: RunContext) -> str:
        """Gets the CPU temperature of the Raspberry Pi."""
        print("ğŸŒ¡ï¸ [TOOL] get_cpu_temperature called")
        return await self.system_tools.get_cpu_temperature(context)
    
    @function_tool
    async def get_system_info(self, context: RunContext) -> str:
        """Gets comprehensive system information including CPU temperature, usage, memory, disk, and uptime."""
        print("ğŸ’» [TOOL] get_system_info called")
        return await self.system_tools.get_system_info(context)
    
    @function_tool
    async def get_cpu_usage(self, context: RunContext) -> str:
        """Gets the CPU usage percentage."""
        print("âš¡ [TOOL] get_cpu_usage called")
        return await self.system_tools.get_cpu_usage(context)
    
    @function_tool
    async def get_memory_usage(self, context: RunContext) -> str:
        """Gets the memory (RAM) usage information."""
        print("ğŸ§  [TOOL] get_memory_usage called")
        return await self.system_tools.get_memory_usage(context)
    
    @function_tool
    async def ask_about_events(self, question: str, context: RunContext) -> str:
        """
        Answers questions about campus events by searching poster information.
        Use for questions like: "When is the art exhibition?", "What events are happening?", "Where is the sports meet?"
        
        Args:
            question: The question about events to answer
        """
        print(f"ğŸ“… [TOOL] ask_about_events called: {question}")
        
        if not self.event_db:
            return "Event database is not available. I can't answer event questions right now."
        
        results = self.event_db.query(question, n_results=3)
        
        if not results:
            return "I don't have information about that event. Try asking about art exhibition, freshers sportmeet, or openhouse."
        
        # Format response
        responses = []
        for r in results:
            parts = [f"**{r['name']}**"]
            if r.get('date'):
                parts.append(f"Date: {r['date']}")
            if r.get('time'):
                parts.append(f"Time: {r['time']}")
            if r.get('venue'):
                parts.append(f"Venue: {r['venue']}")
            responses.append(" - ".join(parts))
        
        return "Here's what I found:\\n" + "\\n".join(responses)

# Global services (shared across all agent instances)
_global_face_monitor = None
_global_image_server = None
_global_event_db = None
_is_ready = False  # Flag to track if heavy components are loaded

def _load_known_faces():
    """Load face encodings from file"""
    known_faces = {}
    encodings_path = Path(__file__).parent / "known_faces" / "encodings.pkl"
    
    if encodings_path.exists():
        try:
            with open(encodings_path, 'rb') as f:
                known_faces = pickle.load(f)
            print(f"âœ… Loaded face encodings for {len(known_faces)} people: {list(known_faces.keys())}")
        except Exception as e:
            print(f"âš ï¸ Error loading face encodings: {e}")
    else:
        print("âš ï¸ No face encodings found.")
    
    return known_faces

def _init_lightweight():
    """Lightweight init - only start fast services for immediate connection"""
    global _global_image_server
    
    # Start image server for posters/maps (fast)
    if _global_image_server is None:
        assets_dir = Path(__file__).parent / "assets"
        _global_image_server = ImageServer(assets_dir, port=8080)
        _global_image_server.start()
        print("âœ… Image server started")
    
    # Start OLED display (I2C must run on main thread, but it's fast)
    try:
        oled_display.setup_and_start_display()
        print("âœ… OLED display started")
    except Exception as e:
        print(f"âš ï¸ Could not start OLED display: {e}")

async def _init_heavy_async(agent):
    """Background initialization of heavy ML components"""
    global _global_face_monitor, _global_event_db, _is_ready
    
    print("ğŸ”„ Starting background initialization of ML components...")
    
    # Run heavy init in thread pool to not block event loop
    loop = asyncio.get_event_loop()
    
    # 2. Build event database from posters (OCR) - can be slow
    if _global_event_db is None:
        try:
            assets_dir = Path(__file__).parent / "assets"
            _global_event_db = await loop.run_in_executor(
                None, build_event_database, assets_dir
            )
            print("âœ… Event database ready")
        except Exception as e:
            print(f"âš ï¸ Could not build event database: {e}")
            _global_event_db = None
    
    # 3. Start camera and face recognition (slowest)
    if _global_face_monitor is None:
        print("ğŸ¥ Starting camera...")
        known_faces = await loop.run_in_executor(None, _load_known_faces)
        _global_face_monitor = FaceMonitor(known_faces)
        await loop.run_in_executor(None, _global_face_monitor.start)
        await asyncio.sleep(1)  # Give camera time to warm up
        print("âœ… Camera ready!")
    
    # Update agent with initialized components
    agent.face_monitor = _global_face_monitor
    agent.known_faces = _global_face_monitor.known_faces
    agent.event_db = _global_event_db
    
    _is_ready = True
    print("ğŸ‰ All components initialized!")


def _handle_signal(sig, frame):
    """Handle termination signals for graceful shutdown"""
    print(f"\nğŸ›‘ Received signal {sig}, shutting down...")
    try:
        if oled_display.DISPLAY_RUNNING:
            print("ğŸ‘‹ OLED: Shutdown requested via signal")
            oled_display.stop_display()
    except Exception as e:
        print(f"âš ï¸ Shutdown signal error: {e}")
    
    # Allow natural exit
    # sys.exit(0) is not needed as LiveKit runner handles it, but we ensured OLED stop


async def entrypoint(ctx: agents.JobContext):
    global _global_face_monitor, _global_image_server, _global_event_db, _is_ready
    
    # Register signal handlers for Ctrl+C and termination
    loop = asyncio.get_running_loop()
    for sig in (signal.SIGINT, signal.SIGTERM):
        try:
            loop.add_signal_handler(sig, lambda: asyncio.create_task(shutdown_wrapper()))
        except (NotImplementedError, ValueError):
            # Fallback for systems where add_signal_handler isn't available
            signal.signal(sig, _handle_signal)
            
    async def shutdown_wrapper():
        """Clean shutdown transition"""
        print("ğŸ§¼ Performing final cleanup...")
        try:
            if oled_display.DISPLAY_RUNNING:
                oled_display.stop_display()
        except:
            pass
        # Give a small moment for I2C to settle
        await asyncio.sleep(0.5)
        # Note: We don't exit here, we let the runner clean up the rest

    
    # LIGHTWEIGHT init - only start fast services
    _init_lightweight()
    
    # Create session immediately (no waiting for ML models)
    session = AgentSession(
        stt=deepgram.STT(model="nova-2"),
        tts=deepgram.TTS(model="aura-2-luna-en"),
        vad=silero.VAD.load(),
        llm=openai.LLM(
            base_url="https://openrouter.ai/api/v1",
            api_key=os.getenv("OPENROUTER_API_KEY"),
            model=os.getenv("LLM_CHOICE", "mistralai/devstral-2512:free"),
        ),
    )
    
    # Create agent without heavy components (will be set later)
    agent = CampusGreetingAgent(_global_image_server, None)  # event_db set later
    agent.room = ctx.room
    agent.face_monitor = None  # Will be set after background init
    
    # Context Injection: LLM always knows who's in front (handles None face_monitor)
    async def inject_person_context(assistant: AgentSession, chat_ctx):
        # Check if face monitor is ready
        if not _is_ready or agent.face_monitor is None:
            from livekit.agents.llm import ChatMessage, ChatRole
            context_msg = ChatMessage(
                role=ChatRole.SYSTEM,
                content="System is still initializing. Face recognition not yet available."
            )
            chat_ctx.messages.insert(0, context_msg)
            return chat_ctx
            
        # Use thread-safe FRESH people getter (most recent detection)
        fresh = agent.face_monitor.get_fresh_people()
        
        # Categorize
        known = [p for p in fresh if p != "Unknown"]
        unknown_count = sum(1 for p in fresh if p == "Unknown")
        
        from livekit.agents.llm import ChatMessage, ChatRole
        
        # Debug: log what we're injecting
        print(f"ğŸ¯ Context injection - Fresh: {fresh}, Known: {known}")
        
        if known:
            names = ", ".join(known)
            if unknown_count:
                context_msg = ChatMessage(
                    role=ChatRole.SYSTEM,
                    content=f"CURRENT PERSON IN FRONT OF YOU: {names}. There's also someone you don't recognize. When asked 'who am I', answer with: {names}"
                )
            else:
                context_msg = ChatMessage(
                    role=ChatRole.SYSTEM,
                    content=f"CURRENT PERSON IN FRONT OF YOU: {names}. When asked 'who am I', answer with: {names}"
                )
        elif unknown_count:
            context_msg = ChatMessage(
                role=ChatRole.SYSTEM,
                content="CURRENT: Unknown person. You don't recognize them. Ask for their name."
            )
        else:
            context_msg = ChatMessage(
                role=ChatRole.SYSTEM,
                content="No one is visible right now."
            )
        
        chat_ctx.messages.insert(0, context_msg)
        return chat_ctx
        
    session.before_llm_cb = inject_person_context
    
    # User speech listener: Show idle2 when user is talking
    async def on_user_speech(ev):
        """Handle user speech events - show idle2 when user is speaking"""
        try:
            if oled_display.DISPLAY_RUNNING:
                if hasattr(ev, 'is_speaking'):
                    if ev.is_speaking:
                        # User started speaking - show idle2 (attentive/listening)
                        oled_display.start_emotion("idle2")
                        print("ğŸ‘‚ User speaking - showing idle2")
                    else:
                        # User stopped speaking - return to idle1
                        oled_display.stop_emotion()
                        print("ğŸ‘€ User stopped - returning to idle1")
        except Exception as e:
            print(f"âš ï¸ User speech OLED error: {e}")
    
    # Proactive Greeting Task: Watch for new people (only runs after init completes)
    async def monitor_and_greet():
        """Background task that greets people when they appear"""
        # Wait for initialization to complete
        while not _is_ready:
            await asyncio.sleep(1)
        
        await asyncio.sleep(2)  # Additional delay after init
        print("ğŸ”„ Background greeting monitor started (multi-person mode)")
        
        while ctx.room.connection_state == rtc.ConnectionState.CONN_CONNECTED:
            try:
                if agent.face_monitor is None:
                    await asyncio.sleep(2)
                    continue
                    
                # Get new arrivals (FaceMonitor handles 5-second cooldown)
                arrivals = agent.face_monitor.get_new_arrivals()
                
                if arrivals:
                    print(f"ğŸ‘‹ New arrivals: {arrivals}")
                    
                    # Categorize arrivals
                    known_people = [p for p in arrivals if p != "Unknown"]
                    unknown_count = arrivals.count("Unknown")
                    
                    # Mark all as greeted
                    for p in arrivals:
                        agent.face_monitor.mark_greeted(p)
                    
                    try:
                        if len(known_people) > 0 and unknown_count == 0:
                            if len(known_people) == 1:
                                name = known_people[0]
                                greeting = generate_greeting(name, is_known=True)
                                print(f"âœ… Greeting known person: {name} -> {greeting}")
                                await emotional_say(session, greeting)
                            else:
                                greeting = generate_group_greeting(known_people, 0)
                                print(f"âœ… Greeting multiple known people -> {greeting}")
                                await emotional_say(session, greeting)
                        
                        elif known_people and unknown_count > 0:
                            greeting = generate_group_greeting(known_people, unknown_count)
                            print(f"ğŸ¤” Greeting mix -> {greeting}")
                            await emotional_say(session, greeting)
                        
                        elif unknown_count == 1:
                            greeting = generate_greeting("Unknown", is_known=False)
                            print(f"ğŸ¤” Greeting unknown person -> {greeting}")
                            await emotional_say(session, greeting)
                        
                        else:
                            greeting = generate_group_greeting([], unknown_count)
                            print(f"ğŸ‘¥ Greeting {unknown_count} unknown people -> {greeting}")
                            await emotional_say(session, greeting)
                            
                    except RuntimeError:
                        print("âš ï¸ Session closing, stopping greetings")
                        break
                        
            except Exception as e:
                print(f"âš ï¸ Greeting error: {e}")
                import traceback
                traceback.print_exc()
                
            await asyncio.sleep(2)
    
        # --- Register event listeners BEFORE session.start() ---
        
        # Register user state callback for idle2 (listening) emotion
        @session.on("user_state_changed")
        def on_user_state_changed(state):
            """Show idle2 when user is speaking, idle1 when stopped"""
            try:
                if oled_display.DISPLAY_RUNNING:
                    if state.speaking:
                        oled_display.start_emotion("idle2")
                        print("ğŸ‘‚ User speaking - showing idle2")
                    else:
                        oled_display.stop_emotion()
                        print("ğŸ‘€ User stopped - returning to idle1")
            except Exception as e:
                print(f"âš ï¸ User state OLED error: {e}")

        # Precise emotion finish listeners
        @session.on("agent_speech_stopped")
        @session.on("agent_speech_finished")
        def on_agent_speech_finished(ev):
            print(f"ğŸ”Š Session: Speech finish event fired")
            try:
                if oled_display.DISPLAY_RUNNING:
                    oled_display.stop_emotion()
                    print("ğŸ‘€ OLED: Returned to idle")
            except Exception as e:
                print(f"âš ï¸ Speech finish OLED error: {e}")

        @session.on("agent_speech_interrupted")
        def on_agent_speech_interrupted(ev):
            print("ğŸ”Š Session: agent_speech_interrupted event fired")
            try:
                if oled_display.DISPLAY_RUNNING:
                    oled_display.stop_emotion()
                    print("ğŸ‘€ OLED: Interrupted - requested idle")
            except Exception as e:
                print(f"âš ï¸ Agent speech interrupt OLED error: {e}")

        # START SESSION
        print("ğŸš€ Starting LiveKit session...")
        await session.start(room=ctx.room, agent=agent)
        
        
        # Send loading message right away
        print("ğŸ’¬ Sending loading message...")
        await session.say("Give me a moment to wake up. I'm loading my systems...")
        
        # Start background initialization
        print("ğŸ”„ Starting background initialization...")
        await _init_heavy_async(agent)
        
        # Announce readiness
        print("ğŸ‰ Initialization complete - announcing readiness")
        await session.say("I'm ready! How can I help you today?")
        
        # NOW start background greeting monitor
        asyncio.create_task(monitor_and_greet())
        
        # Keep session alive
        while ctx.room.connection_state == rtc.ConnectionState.CONN_CONNECTED:
            await asyncio.sleep(1)
        
    finally:
        # CLEANUP - Show sad emotion on disconnect, then shutdown
        print("ğŸ”Œ Session ending...")
        
        # Show sad emotion when disconnecting
        try:
            if oled_display.DISPLAY_RUNNING:
                print("ğŸ˜¢ Showing sad emotion for disconnect...")
                oled_display.display_emotion("sad")
                await asyncio.sleep(2)  # Let it play for 2 seconds
                oled_display.stop_display()
                print("ğŸ‘€ OLED display stopped safely")
        except Exception as e:
            print(f"âš ï¸ OLED shutdown error: {e}")
        
        # Release camera
        if agent.face_monitor:
            agent.face_monitor.stop()
            print("ğŸ“· Camera released")

if __name__ == "__main__":
    agents.cli.run_app(agents.WorkerOptions(
        entrypoint_fnc=entrypoint,
        agent_name="campus-greeting-agent",  # Must match frontend AGENT_NAME
        initialize_process_timeout=120,  # 2 minutes for slow devices like Raspberry Pi
    ))
